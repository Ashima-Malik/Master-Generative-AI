{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Models"
      ],
      "metadata": {
        "id": "Pz87RWoExOF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two main types of models that LangChain integrates with: LLMs and Chat Models."
      ],
      "metadata": {
        "id": "CemZ6KdzxUrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMs"
      ],
      "metadata": {
        "id": "4IC5chudxVRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function:**LLMs are powerful AI models trained on massive amounts of text data. They can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
        "\n",
        "**Focus:** LLMs excel at understanding and manipulating language in a comprehensive way. They can handle various tasks like summarization, question answering, and text generation based on the prompt or input provided.\n",
        "\n",
        "**Integration:**In Langchain, you interact with LLMs through the LLMChain class. You define the LLM model you want to use (e.g., pretrained models from providers like OpenAI or Hugging Face) and provide a prompt or input text as a string. The LLMChain then interacts with the chosen LLM and retrieves the generated output as a string."
      ],
      "metadata": {
        "id": "CRfoCDyCxYrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Models"
      ],
      "metadata": {
        "id": "DnJY3QIcxw13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function:** Chat models are specialized for conversational interactions. They take a sequence of messages (like a chat history) as input and generate a response that fits the conversation context.\n",
        "\n",
        "**Focus:** Chat models are designed to understand the flow of conversation, consider previous messages, and respond accordingly. They are ideal for building chatbots or virtual assistants that can engage in natural and engaging dialogues.\n",
        "\n",
        "**Integration:** Langchain provides pre-built chat models for various functionalities like information retrieval or factual question answering. You can use these models directly or create custom chat models using Langchain's API."
      ],
      "metadata": {
        "id": "AYaMht0_xvTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When to Use Which?**\n",
        "\n",
        "Use LLMs when you need general-purpose text processing capabilities like summarization, question answering, or creative text generation.\n",
        "\n",
        "Use Chat Models when you're building conversational AI agents that need to engage in back-and-forth dialogue and understand the context of a conversation."
      ],
      "metadata": {
        "id": "coLJG5HzyAIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Messages"
      ],
      "metadata": {
        "id": "OubNZX3yyN0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Messages are fundamental units that represent the information exchanged within a conversation or workflow. They come in various types, each serving a specific purpose in defining the flow of information:\n",
        "\n",
        "Types of Messages in Langchain:\n",
        "\n",
        "**HumanMessage:** This type represents a message originating from the human user interacting with your conversational AI agent. It typically contains the user's query, request, or response within the conversation.\n",
        "\n",
        "**AIMessage:** This type represents a message generated by the AI agent itself. It can be a response to a user's query, the output from a Langchain tool, or any information the agent provides during the interaction.\n",
        "\n",
        "**SystemMessage:** This type is used for internal communication within the Langchain workflow. It doesn't directly appear in the conversation with the user but provides instructions or configuration details for the agent's behavior."
      ],
      "metadata": {
        "id": "aketnslNy-Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Message Properties:\n",
        "\n",
        "**Content:** This is the main textual information carried by the message.\n",
        "\n",
        "**Additional Arguments (Optional):** You can optionally include additional data associated with the message using a dictionary passed as the additional_kwargs argument. This can be useful for storing context-specific information or metadata."
      ],
      "metadata": {
        "id": "ddmAFX4CzBZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompts"
      ],
      "metadata": {
        "id": "HJSKyw_bzKSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts are crucial elements that guide Large Language Models (LLMs) and other tools towards generating the desired output. They act as instructions or input that set the context and parameters for the model's operation.\n",
        "\n",
        "**Function of Prompts:**\n",
        "\n",
        "**LLM Interactions:** For LLMs, prompts specify the task at hand and the data they should process. They can be simple questions, instructions for different creative text formats, or more complex prompts for tasks like summarization or question answering. The quality and clarity of the prompt significantly influence the quality of the LLM's output.\n",
        "\n",
        "**Tool Guidance:** Prompts can also be used to guide other Langchain tools. For example, a prompt for a web search tool might specify the keywords to search for.\n",
        "\n",
        "*Structure of a Well-Constructed Prompt:*\n",
        "\n",
        "An effective prompt typically includes the following elements:\n",
        "\n",
        "**Instructions:** Define the desired outcome or task for the LLM or tool.\n",
        "\n",
        "**Context:** Provide relevant background information or details that help the model understand the situation. This could involve providing snippets of text, referencing previous conversation history, or specifying any relevant entities.\n",
        "\n",
        "**User Input (Optional):** In some cases, prompts might incorporate placeholders for user input captured during the conversation. This allows for personalization and dynamic responses based on the user's specific query.\n",
        "\n",
        "**Output Indicator (Optional):** Sometimes, prompts can include a specific marker to indicate where the LLM's generated response should begin within the overall output.\n",
        "\n",
        "**Benefits of Good Prompt Engineering:**\n",
        "\n",
        "Improved Output Quality: Clear and well-designed prompts lead to more accurate, relevant, and informative responses from LLMs and tools.\n",
        "\n",
        "Enhanced Control: By carefully crafting prompts, you can exercise greater control over the direction and style of the generated text.\n",
        "\n",
        "Flexibility: Prompts allow you to adapt your workflows to handle various tasks and user interactions effectively.\n"
      ],
      "metadata": {
        "id": "NZKV2Nad0Med"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain provides tooling to create and work with prompt templates."
      ],
      "metadata": {
        "id": "RnqTZqF0358Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgTZJLWe7sWQ",
        "outputId": "1ce19575-6a14-4e7a-ad54-2fe959198126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.12-py3-none-any.whl (809 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.28 (from langchain)\n",
            "  Downloading langchain_community-0.0.28-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.31 (from langchain)\n",
            "  Downloading langchain_core-0.1.32-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.27-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (3.7.1)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.31->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.12 langchain-community-0.0.28 langchain-core-0.1.32 langchain-text-splitters-0.0.1 langsmith-0.1.27 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnwzRCYRlN3X",
        "outputId": "a2c32cc9-497d-4c67-cf47-83df9793a49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.27 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.30)\n",
            "Collecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (0.1.23)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.27->langchain_openai) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain_openai) (2.0.7)\n",
            "Installing collected packages: h11, tiktoken, httpcore, httpx, openai, langchain_openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 langchain_openai-0.0.8 openai-1.13.3 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irR7MTeu469W",
        "outputId": "3e4ae2cf-a2d8-40b2-82d3-67eac2be147d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "c2W7F6_mxQkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPEN AI API KEY\""
      ],
      "metadata": {
        "id": "IccSl5rjlAs4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAI"
      ],
      "metadata": {
        "id": "GwuTTq8r78x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()\n",
        "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ewmyBGJVlUHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM objects take string as input and output string. The ChatModel objects take a list of messages as input and output a message."
      ],
      "metadata": {
        "id": "JBl3vpcPljJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "text = \"Suggest me a good company name for digital marketinng service agency?\"\n",
        "messages = [HumanMessage(content=text)]\n",
        "\n",
        "llm.invoke(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "roLFp3tklcMh",
        "outputId": "481255bb-6d04-44de-9b6f-1c9f4c40fe1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Digital Ninjas \\n2. MarketBoosters \\n3. ClickGenius \\n4. PixelPro \\n5. SocialSphere \\n6. DigitalStorm \\n7. WebWizards \\n8. BrandBoosters \\n9. The Digital Hive \\n10. MarketMasters \\n11. Digitally Savvy \\n12. The Digital Gang \\n13. ClickSquad \\n14. Digital Impact \\n15. MarketGenius \\n16. WebWise Solutions \\n17. DigitalCrafters \\n18. The Brand Beacon \\n19. MarketMinds \\n20. Click Catalyst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyQtuc-zl513",
        "outputId": "9b4a0f86-941d-4442-934e-7514e62ec6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='1. DigitalBoost Agency\\n2. MarketMavens\\n3. ClickConvert Solutions\\n4. Amplify Digital Agency\\n5. PixelPerfect Marketing\\n6. TrendTech Solutions\\n7. Digital Dynamo Agency\\n8. Impactful Marketing Co.\\n9. Digital Drive Agency\\n10. BrandBloom Marketing')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM returns a string, while the ChatModel returns a message."
      ],
      "metadata": {
        "id": "qtQ4XKcZl7l6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we dont want to pass user input directly to LLM, in that case we can add the user input directly to the prompt template.\n",
        "\n",
        "In previous example, we passed the text to generate the name for digital marketing service agency, but suppose, now we dont want to provide new text everytime to generate names for any agency. Then we can use Prompt Template without worrying about giving the model instructions."
      ],
      "metadata": {
        "id": "B2xML1oumFD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Suggest me a good company name for {agency}?\")\n",
        "prompt.format(agency=\"digital marketinng service agency\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_7CLYHwNnNy8",
        "outputId": "3c5960ba-ec19-496c-f379-363f267a5f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Suggest me a good company name for digital marketinng service agency?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use PromptTemplate to create a template for string prompt."
      ],
      "metadata": {
        "id": "JaHUmaybnkdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI assistant bot. Your name is {name}.\"),\n",
        "        (\"human\", \"Hi, how are you?\"),\n",
        "        (\"ai\", \"I am doing good\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(name=\"Lily\", user_input=\"What is your name?\")"
      ],
      "metadata": {
        "id": "NdI_j0ixoCAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptgXOqmJl9Xq",
        "outputId": "359254f8-009f-437e-f35e-1649e89a47e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful AI assistant bot. Your name is Lily.'),\n",
              " HumanMessage(content='Hi, how are you?'),\n",
              " AIMessage(content='I am doing good'),\n",
              " HumanMessage(content='What is your name?')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selectors"
      ],
      "metadata": {
        "id": "PfpIyRx1pCe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different selector types available, each with its own criteria for choosing a template:\n",
        "\n",
        "**By Model:** This selector chooses a template based on the specific generative AI model you're using. Different models might have varying prompt requirements.\n",
        "\n",
        "**By Length:** This selector picks a template based on a desired output length. For instance, if you want a short summary, it might choose a template concisely conveying the main idea.\n",
        "\n",
        "**By Maximal Marginal Relevance (MMR):** This selector focuses on maximizing the difference between relevant information and redundant content across multiple candidate templates.\n",
        "\n",
        "**By N-gram Overlap:** This selector considers the overlap in word sequences (n-grams) between the input data and the template options. The template with the highest n-gram overlap is likely chosen.\n",
        "\n",
        "**By Similarity:** This selector employs similarity metrics to compare the input data with each template. The most similar template is then selected.\n",
        "\n",
        "**Benefits of Selector Types:**\n",
        "\n",
        "Improved Performance: By selecting the most fitting template for the situation, selector types can potentially lead to better quality outputs from the generative AI model.\n",
        "\n",
        "Flexibility: They allow you to create a collection of prompt templates for various use cases without needing to manually choose each time.\n",
        "\n",
        "Simplified Workflow: Selector types automate the prompt selection process, saving you time and effort."
      ],
      "metadata": {
        "id": "nzG4XO7opD6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### select by length"
      ],
      "metadata": {
        "id": "2W-hSguEp64W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In FewShotPromptTemplate, we provide the examples to the LLM model. It will tell the LLM about the context and how we want the output to look like."
      ],
      "metadata": {
        "id": "ds6yjRNvvD1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "examples = [\n",
        "    {\"input\": \"parrot\", \"output\": \"parrot = bird\"},\n",
        "]\n",
        "example_prompt = PromptTemplate.from_template(\n",
        "  \"Word:{input}\\nExplanation:{output}\"\n",
        ")\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    suffix=\"Word:{input}\",\n",
        "    input_variables=[\"input\"],\n",
        ")\n",
        "\n",
        "llm.invoke(prompt.format(input=\"bat\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N7Suf3ZtwI3c",
        "outputId": "c36839ce-68ff-4365-a91e-7b87c8e436aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nExplanation:bat = mammal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want LLM to understand the bat as an animal not as cricket bat. So, we provided the example to understand it."
      ],
      "metadata": {
        "id": "FNdz-fJ7xU1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, it will get complicated when we have large no of examples. If we send redundant no. of examples, it will not only cost to our API call but also produce the wrong results. So, Langchain privdes example selectors to select the relevant examples based on certain conditions."
      ],
      "metadata": {
        "id": "KOn6uNyfxeHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LengthBasedExampleSelector will select the examples based on length. Each of the word in the example including the variable declaration is count as length one."
      ],
      "metadata": {
        "id": "7ToRJ-TCzE0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "# Examples of a pretend task of creating antonyms.\n",
        "examples = [\n",
        "    {\"input\": \"parrot\", \"output\": \"bird\"},\n",
        "    {\"input\": \"cow\", \"output\": \"animal\"},\n",
        "    {\"input\": \"snake\", \"output\": \"reptile\"},\n",
        "    {\"input\": \"frog\", \"output\": \"amphibian\"},\n",
        "    {\"input\": \"dog\", \"output\": \"mammal\"},\n",
        "]\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=10,\n",
        ")\n",
        "dynamic_prompt = FewShotPromptTemplate(example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"classify every input\",\n",
        "    suffix=\"Input: {name}\\nOutput:\",\n",
        "    input_variables=[\"classify\"],)\n"
      ],
      "metadata": {
        "id": "ot-lxQXZxbvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dynamic_prompt.format(name=\"bat\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy5PbhKG2V0k",
        "outputId": "c688cf95-7edf-4c5c-d0fc-1a60a34156a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classify every input\n",
            "\n",
            "Input: parrot\n",
            "Output: bird\n",
            "\n",
            "Input: cow\n",
            "Output: animal\n",
            "\n",
            "Input: bat\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### select by maximal marginal relevance (MMR)"
      ],
      "metadata": {
        "id": "l-LswJp72sc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It selects the examples that are most similar to the inputs. It does this by finding the examples with the embeddings that have greatest cosine similarity with the inputs, then iteratively adding them while penalizing them for closeness to already selected examples."
      ],
      "metadata": {
        "id": "2uffQhYp2wJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relevance Score:** Each document or passage is assigned a relevance score based on its similarity to the query or topic of interest. This score can be computed using various techniques such as TF-IDF (Term Frequency-Inverse Document Frequency) or neural embeddings.\n",
        "\n",
        "**Diversity Score:** MMR also calculates a diversity score for each document or passage, which measures its dissimilarity to the documents already selected. This can be computed using metrics like cosine similarity or Jaccard distance.\n",
        "\n",
        "**Combining Scores:** MMR combines the relevance and diversity scores using a parameter called lambda (λ), which controls the trade-off between relevance and diversity. The formula for computing the MMR score for a document or passage is:\n",
        "\n",
        "MMR score = λ * Relevance score - (1-λ) * Diversity score\n",
        "\n",
        "Documents with higher MMR scores are prioritized for inclusion in the final result set.\n",
        "\n",
        "**Greedy Selection:** Starting with an empty set of selected documents or passages, MMR iteratively selects the one with the highest MMR score and adds it to the result set. This process continues until a predetermined number of documents or a stopping criterion is reached."
      ],
      "metadata": {
        "id": "24w8nDRN3wwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why use Lambda?\n",
        "\n",
        "Tuning the Balance: The lambda (λ) acts as a weight between relevance and diversity. By adjusting its value, you can control which factor is prioritized.\n",
        "Flexibility: This approach allows you to fine-tune the selection process based on your specific needs.\n",
        "How it Works:\n",
        "\n",
        "Lambda Value:\n",
        "A lambda close to 1 (e.g., 0.9) emphasizes relevance, ensuring the chosen output closely aligns with the prompt.\n",
        "A lambda closer to 0 (e.g., 0.1) prioritizes diversity, selecting an output that might be less relevant but offers a new perspective or creative direction.\n",
        "Balancing Act: The formula subtracts the diversity score weighted by (1-λ) from the relevance score. This essentially balances the two scores based on the chosen lambda value.\n",
        "Example:\n",
        "\n",
        "Imagine two outputs, A and B:\n",
        "Output A: Highly relevant to the prompt but similar to previous outputs.\n",
        "Output B: Less relevant but offers a unique and interesting perspective.\n",
        "With a high lambda (e.g., 0.9), A would likely be chosen due to its strong relevance.\n",
        "With a low lambda (e.g., 0.1), B might be selected to encourage exploration of new ideas."
      ],
      "metadata": {
        "id": "Z4PjsdWEGDfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain.prompts.example_selector import (\n",
        "    MaxMarginalRelevanceExampleSelector,\n",
        "    SemanticSimilarityExampleSelector,\n",
        ")\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"parrot\", \"output\": \"bird\"},\n",
        "    {\"input\": \"cow\", \"output\": \"animal\"},\n",
        "]"
      ],
      "metadata": {
        "id": "nHYkULqT4DJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
        "    examples,\n",
        "    OpenAIEmbeddings(),\n",
        "    FAISS,\n",
        "    k=2,\n",
        ")\n",
        "mmr_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"classify every input\",\n",
        "    suffix=\"Input:{classify}\\nOutput:\",\n",
        "    input_variables=[\"classify\"],\n",
        ")"
      ],
      "metadata": {
        "id": "VmMMI1SI4R1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mmr_prompt.format(classify=\"bat\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDEV7jK-5DQH",
        "outputId": "1e5e369a-19a8-44d2-b173-ca25770e12c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classify every input\n",
            "\n",
            "Input: parrot\n",
            "Output: bird\n",
            "\n",
            "Input: energetic\n",
            "Output: lethargic\n",
            "\n",
            "Input:bat\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets check what we will get based on SemanticSimilarityExampleSelector."
      ],
      "metadata": {
        "id": "E-tW-s2g5uEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples,\n",
        "    OpenAIEmbeddings(),\n",
        "    FAISS,\n",
        "    k=2,\n",
        ")\n",
        "similar_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"classify every input\",\n",
        "    suffix=\"Input:{classify}\\nOutput:\",\n",
        "    input_variables=[\"classify\"],\n",
        ")"
      ],
      "metadata": {
        "id": "GQ4IYSH-5xOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(similar_prompt.format(classify=\"bat\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5C3QRUh57EZ",
        "outputId": "6ae5aa41-ad78-4795-90d3-ad2e0e63297e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classify every input\n",
            "\n",
            "Input: parrot\n",
            "Output: bird\n",
            "\n",
            "Input: cow\n",
            "Output: animal\n",
            "\n",
            "Input:bat\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### select by n-gram overlap"
      ],
      "metadata": {
        "id": "_FiKGQnG6Eh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Threshold Score | Behavior                                                                                                                                                  |\n",
        "|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| -1.0            | Default threshold; selects all examples but sorts them in descending order based on their n-gram score.                                                   |\n",
        "| 1.0             | Excludes all examples and returns an empty list.                                                                                                          |\n",
        "| 0.0             | Excludes examples with zero n-gram overlap with the input.                                                                                                |\n",
        "| 0.0 < n < 1.0   | Excludes examples with n-gram overlap less than the specified threshold 'n'.                                                                              |\n"
      ],
      "metadata": {
        "id": "daXfi0eX6NVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector.ngram_overlap import NGramOverlapExampleSelector\n"
      ],
      "metadata": {
        "id": "ERHtygC194D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\",\n",
        ")\n",
        "\n",
        "# Examples of a fictional translation task.\n",
        "examples = [{\"input\": \"What is the capital of France?\", \"output\": \"The capital of France is Paris.\"},\n",
        "{\"input\": \"Who wrote 'Romeo and Juliet'?\", \"output\": \"William Shakespeare wrote 'Romeo and Juliet'.\"},\n",
        "{\"input\": \"How many continents are there?\", \"output\": \"There are seven continents on Earth.\"},\n",
        "{\"input\": \"What is the boiling point of water?\", \"output\": \"The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit).\"},\n",
        "{\"input\": \"Who was the first person to walk on the moon?\", \"output\": \"Neil Armstrong was the first person to walk on the moon.\"},\n",
        "]\n",
        "\n",
        "example_selector = NGramOverlapExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    threshold=0,\n",
        " )\n",
        "ngram_prompt = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"question to answer\",\n",
        "    suffix=\"Input: {question}\\nOutput:\",\n",
        "    input_variables=[\"question\"],\n",
        ")\n",
        "\n",
        "print(ngram_prompt.format(question=\"Which city is known as the City of Love?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-b8R_r095A6",
        "outputId": "fceb909d-239d-4de0-8122-944a42907dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question to answer\n",
            "\n",
            "Input: What is the capital of France?\n",
            "Output: The capital of France is Paris.\n",
            "\n",
            "Input: What is the boiling point of water?\n",
            "Output: The boiling point of water is 100 degrees Celsius (212 degrees Fahrenheit).\n",
            "\n",
            "Input: Who was the first person to walk on the moon?\n",
            "Output: Neil Armstrong was the first person to walk on the moon.\n",
            "\n",
            "Input: Which city is known as the City of Love?\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PipelinePromptTemplate"
      ],
      "metadata": {
        "id": "PKTgohqqASNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we want to compose multiple prompts together and want to reuse parts of prompts. This can be done with a PipelinePrompt.\n",
        "\n",
        "1. Final prompt: The final prompt that is returned\n",
        "\n",
        "2. Pipeline prompts: A list of tuples, consisting of a string name and a prompt template.\n",
        "\n",
        "Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name."
      ],
      "metadata": {
        "id": "w0pL08D1AVd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
        "from langchain.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "2TzeijviAuzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_template = \"\"\"{introduction}\n",
        "\n",
        "{example}\n",
        "\n",
        "{start}\"\"\""
      ],
      "metadata": {
        "id": "H5yvjvuNBBbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_prompt = PromptTemplate.from_template(complete_template)"
      ],
      "metadata": {
        "id": "Qdl8JQQOBhey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "introduction_template = \"\"\"You are impersonating {person}.\"\"\"\n",
        "introduction_prompt = PromptTemplate.from_template(introduction_template)"
      ],
      "metadata": {
        "id": "jdkgYt2PBl7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_template = \"\"\"Here's an example of an interaction:\n",
        "\n",
        "Q: {example_q}\n",
        "A: {example_a}\"\"\"\n",
        "example_prompt = PromptTemplate.from_template(example_template)"
      ],
      "metadata": {
        "id": "WuZ01Xz_Bri0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_template = \"\"\"Now, do this for real!\n",
        "\n",
        "Q: {input}\n",
        "A:\"\"\"\n",
        "start_prompt = PromptTemplate.from_template(start_template)"
      ],
      "metadata": {
        "id": "SXbJ-V6QBxNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompts = [\n",
        "    (\"introduction\", introduction_prompt),\n",
        "    (\"example\", example_prompt),\n",
        "    (\"start\", start_prompt),\n",
        "]\n",
        "pipeline_prompt = PipelinePromptTemplate(\n",
        "    final_prompt=complete_prompt, pipeline_prompts=input_prompts\n",
        ")"
      ],
      "metadata": {
        "id": "ZYWq7iZoBzpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt06gZWVB9d2",
        "outputId": "65cb1533-6e7f-4b09-fa42-70448ba498d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['example_q', 'person', 'example_a', 'input']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    pipeline_prompt.format(\n",
        "        person=\"Elon Musk\",\n",
        "        example_q=\"What's your favorite car?\",\n",
        "        example_a=\"Tesla\",\n",
        "        input=\"What's your favorite social media site?\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AhOjKwGB-Hk",
        "outputId": "ef86533c-deb8-4069-c3e3-709d45fddd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are impersonating Elon Musk.\n",
            "\n",
            "Here's an example of an interaction:\n",
            "\n",
            "Q: What's your favorite car?\n",
            "A: Tesla\n",
            "\n",
            "Now, do this for real!\n",
            "\n",
            "Q: What's your favorite social media site?\n",
            "A:\n"
          ]
        }
      ]
    }
  ]
}