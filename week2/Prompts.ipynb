{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa7xynBJSSmj",
        "outputId": "69aa411c-f54f-4ad8-8714-96d7bc12a8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.47-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.32 langchain-core-0.1.42 langchain-text-splitters-0.0.1 langsmith-0.1.47 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.0 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.3-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.42)\n",
            "Collecting openai<2.0.0,>=1.10.0 (from langchain_openai)\n",
            "  Downloading openai-1.17.1-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (0.1.47)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (2.6.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.42->langchain_openai) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain_openai) (3.10.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain_openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain_openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.0.7)\n",
            "Installing collected packages: h11, tiktoken, httpcore, httpx, openai, langchain_openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 langchain_openai-0.1.3 openai-1.17.1 tiktoken-0.6.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install langchain_openai\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompts"
      ],
      "metadata": {
        "id": "mlMWxbcOS1eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Messages are fundamental units that represent the information exchanged within a conversation or workflow. They come in various types, each serving a specific purpose in defining the flow of information:\n",
        "\n",
        "**Types of Messages in Langchain:**\n",
        "\n",
        "**HumanMessage:** This type represents a message originating from the human user interacting with your conversational AI agent. It typically contains the user's query, request, or response within the conversation.\n",
        "\n",
        "**AIMessage:** This type represents a message generated by the AI agent itself. It can be a response to a user's query, the output from a Langchain tool, or any information the agent provides during the interaction.\n",
        "\n",
        "**SystemMessage:** This type is used for internal communication within the Langchain workflow. It doesn't directly appear in the conversation with the user but provides instructions or configuration details for the agent's behavior."
      ],
      "metadata": {
        "id": "Jpr1UYx2S3vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Message Properties:**\n",
        "\n",
        "**Content:** This is the main textual information carried by the message.\n",
        "\n",
        "**Additional Arguments (Optional):** You can optionally include additional data associated with the message using a dictionary passed as the additional_kwargs argument. This can be useful for storing context-specific information or metadata."
      ],
      "metadata": {
        "id": "VIz5ZkHgS6Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts are crucial elements that guide Large Language Models (LLMs) and other tools towards generating the desired output. They act as instructions or input that set the context and parameters for the model's operation.\n",
        "\n",
        "**Function of Prompts:**\n",
        "\n",
        "**LLM Interactions:** For LLMs, prompts specify the task at hand and the data they should process. They can be simple questions, instructions for different creative text formats, or more complex prompts for tasks like summarization or question answering. The quality and clarity of the prompt significantly influence the quality of the LLM's output.\n",
        "\n",
        "**Tool Guidance:** Prompts can also be used to guide other Langchain tools. For example, a prompt for a web search tool might specify the keywords to search for.\n",
        "\n",
        "**Structure of a Well-Constructed Prompt:**\n",
        "\n",
        "An effective prompt typically includes the following elements:\n",
        "\n",
        "**Instructions:** Define the desired outcome or task for the LLM or tool.\n",
        "\n",
        "**Context:** Provide relevant background information or details that help the model understand the situation. This could involve providing snippets of text, referencing previous conversation history, or specifying any relevant entities.\n",
        "\n",
        "**User Input (Optional):** In some cases, prompts might incorporate placeholders for user input captured during the conversation. This allows for personalization and dynamic responses based on the user's specific query.\n",
        "\n",
        "**Output Indicator (Optional):** Sometimes, prompts can include a specific marker to indicate where the LLM's generated response should begin within the overall output.\n",
        "\n",
        "**Benefits of Good Prompt Engineering:**\n",
        "\n",
        "**Improved Output Quality:** Clear and well-designed prompts lead to more accurate, relevant, and informative responses from LLMs and tools.\n",
        "\n",
        "**Enhanced Control:** By carefully crafting prompts, you can exercise greater control over the direction and style of the generated text.\n",
        "\n",
        "**Flexibility:** Prompts allow you to adapt your workflows to handle various tasks and user interactions effectively.\n",
        "\n",
        "LangChain provides tooling to create and work with prompt templates."
      ],
      "metadata": {
        "id": "VdG4_-d3S8ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOU OPENAI API KEY HERE\"\n"
      ],
      "metadata": {
        "id": "YG4Dt63rSd-o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAI"
      ],
      "metadata": {
        "id": "W3SQ_GphTe2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI()\n",
        "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n"
      ],
      "metadata": {
        "id": "svxf4G7UTiRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM objects take string as input and output string. The ChatModel objects take a list of messages as input and output a message."
      ],
      "metadata": {
        "id": "kh5uu-TvTrEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "text = \"Suggest me a good company name for digital marketinng service agency?\"\n",
        "messages = [HumanMessage(content=text)]\n",
        "\n",
        "llm.invoke(text) #invoke llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "agt4bU4RTnxz",
        "outputId": "4db00317-cc08-495f-e77b-45f6902830ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\"Digital Boost Co.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(messages) #invoke chat model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4Ugbw2MUCvZ",
        "outputId": "555aed62-a1b7-41c0-e77c-de048abb8880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='\"ClickBoost Digital\"', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 22, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-9290bb7e-4aa0-4399-bbcf-4fe40389b215-0')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM returns a string, while the ChatModel returns a message.\n",
        "\n",
        "Now, if we dont want to pass user input directly to LLM, in that case we can add the user input directly to the prompt template.\n",
        "\n",
        "In previous example, we passed the text to generate the name for digital marketing service agency, but suppose, now we dont want to provide new text everytime to generate names for any agency. Then we can use Prompt Template without worrying about giving the model instructions."
      ],
      "metadata": {
        "id": "s80ng0SVUMNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Single input variable for LLM"
      ],
      "metadata": {
        "id": "uflLKzoSWeWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"Suggest me a good company name for {agency}?\") #single input variable\n",
        "prompt.format(agency=\"digital marketinng service agency\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3UhIkV6vUIy-",
        "outputId": "0079d61c-0e4a-4e89-aff5-aafbdcab404f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Suggest me a good company name for digital marketinng service agency?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = prompt.format(agency=\"digital marketinng service agency\")"
      ],
      "metadata": {
        "id": "ia9jLRuDU34c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "S-IBcMfKUXjx",
        "outputId": "a22cb9da-6417-4d6d-bb7c-3a248a071017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. DigitalBoost Agency\\n2. MarketMinds Solutions\\n3. PixelPro Marketing\\n4. Bluewave Digital\\n5. NextGen Media Co.\\n6. The Digital Edge Agency\\n7. ClickGenius Marketing\\n8. Thrive Digital Solutions\\n9. MarketMaven Agency\\n10. BuzzBloom Media\\n11. Digital Dynamo Co.\\n12. GrowthGenie Marketing\\n13. The Digital Hive Agency\\n14. MarketMate Solutions\\n15. WebWise Media Co.\\n16. BrandBlasters Agency\\n17. Digital Drive Marketing\\n18. MarketMotion Solutions\\n19. Apex Digital Co.\\n20. Amplify Agency.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiple input variable for LLM"
      ],
      "metadata": {
        "id": "0_xP45JlWiVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_input_prompt = PromptTemplate.from_template(\"What is the {use} of {product}?\") #multiple input variable"
      ],
      "metadata": {
        "id": "3gK3D7vcVqpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_input_prompt.format(use=\"advantage\", product=\"glycerine\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zHUpBZoFV7xa",
        "outputId": "f0e5eaf2-b97d-4564-abf4-ead69e085363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the advantage of glycerine?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multiple_text = multiple_input_prompt.format(use=\"advantage\", product=\"glycerine\")"
      ],
      "metadata": {
        "id": "VpB48kmTWTME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(multiple_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "7yCREdeSWXTi",
        "outputId": "b0a5b4b4-66b0-44d4-fbd1-bf87de45d5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nSome potential advantages of glycerine include its moisturizing properties, its ability to help strengthen and protect the skin barrier, its humectant properties which help to attract and retain moisture, and its potential to improve skin elasticity and smoothness. It may also have antimicrobial and anti-inflammatory effects, making it useful for treating certain skin conditions. Additionally, glycerine is a natural, non-toxic ingredient that is suitable for a variety of skin types.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiple input variable for Chat Model"
      ],
      "metadata": {
        "id": "47II70WBWo-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI assistant bot. Your name is {name}.\"),\n",
        "        (\"human\", \"Hi, how are you?\"),\n",
        "        (\"ai\", \"I am doing good\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(name=\"Lily\", user_input=\"What is your name?\")"
      ],
      "metadata": {
        "id": "kSn55IzAW47a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_gvplqUW868",
        "outputId": "c0d92a87-612b-4b95-fc68-c72c9cffcfba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful AI assistant bot. Your name is Lily.'),\n",
              " HumanMessage(content='Hi, how are you?'),\n",
              " AIMessage(content='I am doing good'),\n",
              " HumanMessage(content='What is your name?')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(messages) #invoke chat model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rinj-sGjW-YL",
        "outputId": "535b0ff2-7c8f-46c6-f7be-84d25fd8f5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='My name is Lily. How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 47, 'total_tokens': 59}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-337fc143-9442-4ca7-a71b-a1ad84fca172-0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate"
      ],
      "metadata": {
        "id": "nJFbWhmqXVsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "pp76MUUoXdD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template=\"You are an helpful AI travel assistant that specializes in suggesting {number} places to visit within {budget}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)"
      ],
      "metadata": {
        "id": "416X_q6UXjZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j35KrFxjYB0L",
        "outputId": "0909f34a-0248-4bdc-c73c-6e448fbe39d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['budget', 'number']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_template=\"{country}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ],
      "metadata": {
        "id": "3RR9PncnYGo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_message_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KTeRrWiYOrl",
        "outputId": "7da90296-c378-42aa-8516-5e1a58782a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['country']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
      ],
      "metadata": {
        "id": "zBrez4JpYSix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sMJWVOdYU1K",
        "outputId": "efcd9cb6-8332-47bc-ded5-29cd6d670361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['budget', 'country', 'number']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get a chat completion from the formatted messages\n",
        "chat_prompt.format_prompt(number=\"5\", budget=\"10,000 dollars\", country=\"USA\").to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLltCCNaYWtJ",
        "outputId": "6f0037a1-eb77-46a9-a085-6b181a97c1b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are an helpful AI travel assistant that specializes in suggesting 5 places to visit within 10,000 dollars.'),\n",
              " HumanMessage(content='USA')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "request = chat_prompt.format_prompt(number=\"5\", budget=\"10,000 dollars\", country=\"USA\").to_messages()"
      ],
      "metadata": {
        "id": "D0C7OuWrY_tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chat_model.invoke(request) #invoke chat model\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTF6IiHBZJmF",
        "outputId": "6876a543-524c-4499-ce17-16f53d88df32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure! Here are 5 places to visit in the USA within a budget of $10,000:\\n\\n1. New York City, New York: Explore the iconic landmarks such as Times Square, Central Park, and the Statue of Liberty. Enjoy Broadway shows, world-class shopping, and diverse culinary experiences.\\n\\n2. Grand Canyon National Park, Arizona: Witness the breathtaking natural beauty of one of the world's most famous canyons. Hike along the rim, take a helicopter tour, or even raft down the Colorado River for a memorable experience.\\n\\n3. New Orleans, Louisiana: Immerse yourself in the vibrant culture of the Big Easy with its jazz music, unique cuisine, and historical architecture. Don't miss out on exploring the French Quarter and attending a live music performance.\\n\\n4. San Francisco, California: Discover the charm of this eclectic city by visiting the Golden Gate Bridge, Alcatraz Island, and Fisherman's Wharf. Take a ride on a cable car, stroll through the colorful neighborhoods, and indulge in delicious seafood.\\n\\n5. Yellowstone National Park, Wyoming: Experience the wonders of nature at America's first national park, home to geysers, hot springs, and diverse wildlife. Go hiking, camping, or wildlife watching for an unforgettable outdoor adventure.\\n\\nThese destinations offer a mix of cultural experiences, natural beauty, and exciting activities within your budget. Enjoy your trip!\", response_metadata={'token_usage': {'completion_tokens': 280, 'prompt_tokens': 35, 'total_tokens': 315}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-e2515daf-2529-41c4-ba89-0cbe6af48d0e-0')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFBn1WHOZbfK",
        "outputId": "1a2250a0-c3cf-4d7d-c71a-2d6918e21b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are 5 places to visit in the USA within a budget of $10,000:\n",
            "\n",
            "1. New York City, New York: Explore the iconic landmarks such as Times Square, Central Park, and the Statue of Liberty. Enjoy Broadway shows, world-class shopping, and diverse culinary experiences.\n",
            "\n",
            "2. Grand Canyon National Park, Arizona: Witness the breathtaking natural beauty of one of the world's most famous canyons. Hike along the rim, take a helicopter tour, or even raft down the Colorado River for a memorable experience.\n",
            "\n",
            "3. New Orleans, Louisiana: Immerse yourself in the vibrant culture of the Big Easy with its jazz music, unique cuisine, and historical architecture. Don't miss out on exploring the French Quarter and attending a live music performance.\n",
            "\n",
            "4. San Francisco, California: Discover the charm of this eclectic city by visiting the Golden Gate Bridge, Alcatraz Island, and Fisherman's Wharf. Take a ride on a cable car, stroll through the colorful neighborhoods, and indulge in delicious seafood.\n",
            "\n",
            "5. Yellowstone National Park, Wyoming: Experience the wonders of nature at America's first national park, home to geysers, hot springs, and diverse wildlife. Go hiking, camping, or wildlife watching for an unforgettable outdoor adventure.\n",
            "\n",
            "These destinations offer a mix of cultural experiences, natural beauty, and exciting activities within your budget. Enjoy your trip!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few Shot Prompt Template"
      ],
      "metadata": {
        "id": "zOzcHlefZ5_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Few-shot prompting is a technique used in large language models (LLMs) to improve their performance on a specific task by providing a few examples beforehand.\n",
        "\n",
        "**Number of Examples:** Typically, a few examples (2-5) are sufficient for few-shot prompting.\n",
        "\n",
        "**Relevance:** Ensure the examples you provide are relevant to the task you want the LLM to perform.\n",
        "\n",
        "**Variety:** Including examples with slight variations can help the LLM generalize better."
      ],
      "metadata": {
        "id": "6_Gr4DN9bOSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "TjOGf8Q6gwbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"You are a helpful assistant that knows everything about the zoology.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "oubM78lnbad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "species_text = \"parrot\"\n",
        "example_input_one = HumanMessagePromptTemplate.from_template(species_text)\n",
        "plain_text = \"It's a bird\"\n",
        "example_output_one = AIMessagePromptTemplate.from_template(plain_text)"
      ],
      "metadata": {
        "id": "DiLBdabefKyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template = \"{input}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ],
      "metadata": {
        "id": "aeF7k8nLfh49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, example_input_one, example_output_one, human_message_prompt]\n",
        ")"
      ],
      "metadata": {
        "id": "jzPQoFa2fpTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"Bat\"\n",
        "request = chat_prompt.format_prompt(input=example_text).to_messages()"
      ],
      "metadata": {
        "id": "Ekm9duUugPDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chat_model.invoke(request)"
      ],
      "metadata": {
        "id": "07EvZH91g8a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asX1E-n8hHJf",
        "outputId": "8249da23-3ba7-4005-b95e-74c36dab6267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bats are mammals that are capable of sustained flight. They are the only mammals naturally capable of true and sustained flight. Bats are found worldwide, except for extreme desert and polar regions. They are nocturnal animals, using echolocation to navigate and hunt for prey. Bats play a crucial role in ecosystems by pollinating flowers, dispersing seeds, and controlling insect populations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Caching"
      ],
      "metadata": {
        "id": "3e3Xp1tDhQxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caching, in computing, refers to the process of storing frequently accessed data in a temporary location for faster retrieval. It's like having a readily available copy of your favorite book next to your reading chair instead of needing to trek to the library every time you want to read it.\n",
        "\n"
      ],
      "metadata": {
        "id": "W5ZH2RaohS0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()\n"
      ],
      "metadata": {
        "id": "JOJi_NhDh4Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.predict(\"Bat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "4MjHTDgxiQG0",
        "outputId": "fbc35285-d824-4d36-a317-f9e425c78852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A bat is a flying mammal that is typically active at night. They use echolocation to navigate and hunt for insects. Bats are important for pollination and pest control in many ecosystems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.predict(\"Bat\") # This reply is instant."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uuIkikGPiUoY",
        "outputId": "e4ca4748-187d-4d31-e2a5-6f407d3f0def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A bat is a flying mammal that is typically active at night. They use echolocation to navigate and hunt for insects. Bats are important for pollination and pest control in many ecosystems.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsers"
      ],
      "metadata": {
        "id": "dexiZ3pZib7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, a parser refers specifically to an output parser. These are tools designed to transform the raw text response generated by a large language model (LLM) into a more structured and usable format.\n",
        "\n",
        "**Why are Output Parsers Used?**\n",
        "\n",
        "**Structured Data Extraction:** Not all tasks require raw text. Sometimes you need the information in a specific format, like JSON, a Python data class, or a Pandas DataFrame. Output parsers can convert the LLM's response into these formats for easier manipulation and use in your workflows.\n",
        "\n",
        "**Key Information Extraction:** For tasks like summarization or question answering, you might only be interested in the most important points. Output parsers can help extract these key details from the LLM's potentially lengthy response.\n",
        "\n",
        "**Data Cleaning & Formatting:** LLM responses might contain inconsistencies or irrelevant details. Output parsers can help clean and format the data to make it consistent and usable for further processing.\n",
        "\n",
        "**Types of Output Parsers in LangChain:**\n",
        "\n",
        "LangChain offers a variety of output parsers for different needs:\n",
        "\n",
        "**Pydantic Parser:** Converts the LLM response into a user-defined Pydantic data class, allowing for structured data manipulation.\n",
        "\n",
        "**JSON Parser:** Transforms the response into a JSON object for easy integration with other tools and workflows.\n",
        "\n",
        "**List Parser:** Parses the output into a list of strings, useful for scenarios where the LLM generates multiple items.\n",
        "\n",
        "**Datetime Parser:** Extracts and formats dates or times mentioned in the response.\n",
        "\n",
        "**Enum Parser:** Converts the response into a pre-defined enumeration type (useful for selecting options).\n",
        "\n",
        "**Structured Parser (Basic):** This is a simpler option that parses the response into a dictionary structure, suitable for less complex data.\n",
        "\n",
        "**Using Output Parsers:**\n",
        "\n",
        "In LangChain, you can incorporate output parsers into your prompt chains.  The specific parser you choose depends on the desired format for the LLM's response.\n",
        "\n",
        "**There are two main ways to use them:**\n",
        "\n",
        "**Within the Prompt:** Some output parsers can be directly integrated into the prompt using specific syntax. This approach allows you to define the parsing logic alongside the prompt itself.\n",
        "\n",
        "**Separate Step:** Alternatively, you can define the output parser as a separate step in the LangChain pipeline. This gives you more flexibility to apply the parser to the raw LLM response after it's generated.\n",
        "\n",
        "Overall, output parsers are a powerful tool in LangChain that bridge the gap between the raw LLM output and your desired data format. They allow you to effectively leverage the capabilities of LLMs for various tasks by structuring and extracting the information you need."
      ],
      "metadata": {
        "id": "f7COfenwijOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CSV parser"
      ],
      "metadata": {
        "id": "OEOhPA4bnU_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List top {number} places to visit in {country}.\\n{format_instructions}\",\n",
        "    input_variables=[\"number\", \"country\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(temperature=0)\n",
        "\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "kzYjDDVWkrPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"number\": \"five\", \"country\": \"India\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k77JM2T4n2zF",
        "outputId": "b7a000a2-7da3-452f-bd5c-9e49fcd49572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Taj Mahal', 'Jaipur', 'Kerala Backwaters', 'Varanasi', 'Goa']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datetime parser"
      ],
      "metadata": {
        "id": "w9GS4XHsoQtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "output_parser = DatetimeOutputParser()\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "prompt = PromptTemplate(\n",
        "    template=\"{question}.\\n{format_instructions}\",\n",
        "    input_variables=[\"question\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(temperature=0)\n",
        "\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "slDltbP6oWTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"When India got independece?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHf3QtPrpSPL",
        "outputId": "fb650141-1c5e-4ab8-86ac-4a81aef8fe62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(1947, 8, 15, 0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pandas DataFrame Parser"
      ],
      "metadata": {
        "id": "jsajOEylp3Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain.output_parsers import PandasDataFrameOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "NTqXaRasp8FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [['Apple', 10.50, 2], ['Banana', 1.25, 5], ['Orange', 2.75, 3]]\n",
        "columns = ['Fruit', 'Price', 'Quantity']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXlcRBzKqTOB",
        "outputId": "270b4cf5-360c-4fa6-d39d-de5315b8c9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Fruit  Price  Quantity\n",
            "0   Apple  10.50         2\n",
            "1  Banana   1.25         5\n",
            "2  Orange   2.75         3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = PandasDataFrameOutputParser(dataframe=df)"
      ],
      "metadata": {
        "id": "wpchIEqkqcw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(temperature=0)\n",
        "\n",
        "chain = prompt | model | output_parser"
      ],
      "metadata": {
        "id": "FyD77mXZqiN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"query\": \"Retrieve the Fruit column\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8otLl3Bq0Xn",
        "outputId": "9d452c93-fb3b-4d96-cd06-9b9f771e968e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Fruit': 0     Apple\n",
              " 1    Banana\n",
              " 2    Orange\n",
              " Name: Fruit, dtype: object}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serialization"
      ],
      "metadata": {
        "id": "3hgxMufN_Aez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"Question: {question}\\n\\nAnswer: step by step solution\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "prompt.save(\"prompt.json\")"
      ],
      "metadata": {
        "id": "RtED_pOj_DDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import load_prompt #load the prompt\n",
        "load_prompt = load_prompt('prompt.json')\n",
        "load_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5_eRipT_X7I",
        "outputId": "40e4dae0-9c45-4ba3-be17-0ac4404e14c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['question'], template='Question: {question}\\n\\nAnswer: step by step solution')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}